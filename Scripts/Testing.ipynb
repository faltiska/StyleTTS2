{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Functions",
   "id": "527970875baff290"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import whisper\n",
    "from Scripts.functions import InferenceResult, length_to_mask\n",
    "from functions import StyleTTS2_Helper\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch # Deep Learning Framework\n",
    "\n",
    "import soundfile as sf\n",
    "from nltk.tokenize import word_tokenize # Tokenizers divide strings into lists of substrings\n",
    "import time # Used for timing operations\n",
    "import yaml\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import whisper\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "\n",
    "import phonemizer\n",
    "\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "os.environ[\"PHONEMIZER_ESPEAK_LIBRARY\"] = r\"C:\\Program Files\\eSpeak NG\\libespeak-ng.dll\"  # <-- adjust if different\n",
    "torch.manual_seed(0) # Fixes starting point of random seed for torch\n",
    "torch.backends.cudnn.benchmark = False # Fix convolution algorithm\n",
    "torch.backends.cudnn.deterministic = True # Only use deterministic algorithms\n",
    "\n",
    "%cd .."
   ],
   "id": "4125e46dc82a6b92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Helper Functions\n",
   "id": "fa992ae834710c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def length_to_mask(lengths):\n",
    "    mask = torch.arange(lengths.max())  # Creates a Vector [0,1,2,3,...,x], where x = biggest value in lengths\n",
    "    mask = mask.unsqueeze(0)  # Creates a Matrix [1,x] from Vector [x]\n",
    "    mask = mask.expand(lengths.shape[0],\n",
    "                       -1)  # Expands the matrix from [1,x] to [y,x], where y = number of elements in lengths\n",
    "    mask = mask.type_as(lengths)  # Assign mask the same type as lengths\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(\n",
    "        1))  # gt = greater than, compares each value from lengths to a row of values in mask; unsqueeze = splits vector lengths into vectors of size 1\n",
    "    return mask  # returns a mask of shape (batch_size, max_length) where mask[i, j] = 1 if j < lengths[i] and mask[i, j] = 0 otherwise."
   ],
   "id": "9b832ba4c05b5db8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def analyzeAudio(wav):\n",
    "    model = whisper.load_model(\"tiny\")\n",
    "    result = model.transcribe(audio=wav)\n",
    "    print(result[\"text\"])\n",
    "\n",
    "    # let's inspect segments\n",
    "    for seg in result[\"segments\"]:\n",
    "        print(\n",
    "            f\"[{seg['start']:.2f} -> {seg['end']:.2f}] \"\n",
    "            f\"text='{seg['text']}' \"\n",
    "            f\"avg_logprob={seg['avg_logprob']:.3f} \"\n",
    "            f\"no_speech_prob={seg['no_speech_prob']:.3f}\"\n",
    "        )"
   ],
   "id": "a1843353fcb860ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generateAudio(pipe, name, text):\n",
    "\n",
    "    inferenceResult = pipe.inference(text, noise=torch.randn(1,1,256).to(pipe.device))\n",
    "    inferenceResult.save(name)\n",
    "    audio = pipe.synthesizeSpeech(inferenceResult)\n",
    "    sf.write(\"outputs/audio/\" + name + \".wav\", audio, samplerate=24000)\n",
    "    return inferenceResult"
   ],
   "id": "7fd0b7466ab98964",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def interpolateAllLatents(to_change, reference, interpolation_percentage):\n",
    "\n",
    "    interpolation_result = {}\n",
    "\n",
    "    for name in reference.__dataclass_fields__:\n",
    "\n",
    "        latent_ground_truth = getattr(reference, name)\n",
    "        latent_target = getattr(to_change, name)\n",
    "\n",
    "        if name != \"h_text\":\n",
    "            interpolation_result[name] = latent_ground_truth\n",
    "            continue\n",
    "\n",
    "        print(\"Starting interpolation for \" + name)\n",
    "        if latent_ground_truth.shape != latent_target.shape:\n",
    "            print(f\"Shape mismatch with ground_truth={latent_ground_truth.shape}, target={latent_target.shape}\")\n",
    "\n",
    "            if (latent_ground_truth.dim() < 3) and (latent_target.dim() < 3):\n",
    "                latent_ground_truth = latent_ground_truth.unsqueeze(1)\n",
    "                latent_target = latent_target.unsqueeze(1)\n",
    "\n",
    "            latent_target = F.interpolate(\n",
    "                input=latent_target,\n",
    "                size=latent_ground_truth.shape[-1],\n",
    "                mode=\"linear\",\n",
    "                align_corners=False\n",
    "            ).squeeze(0)\n",
    "        interpolation_result[name] = latent_ground_truth * (1 - interpolation_percentage) + latent_target * interpolation_percentage\n",
    "\n",
    "    return InferenceResult(**interpolation_result)"
   ],
   "id": "692d11dc8767c112",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def interpolateAttribute(to_change, reference, interpolation_percentage: float):\n",
    "\n",
    "    if reference.shape != to_change.shape:\n",
    "        print(f\"Shape mismatch with ground_truth={reference.shape}, target={to_change.shape}\")\n",
    "\n",
    "        if (reference.dim() < 3) and (to_change.dim() < 3):\n",
    "            reference = reference.unsqueeze(1)\n",
    "            to_change = to_change.unsqueeze(1)\n",
    "\n",
    "        to_change = F.interpolate(\n",
    "            input=to_change,\n",
    "            size=reference.shape[-1],\n",
    "            mode=\"linear\",\n",
    "            align_corners=False\n",
    "        ).squeeze(0)\n",
    "\n",
    "    return reference * (1 - interpolation_percentage) + to_change * interpolation_percentage"
   ],
   "id": "25274fe55e8f09e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def addNoise(reference: InferenceResult, target: InferenceResult, interpolation_percentage: float, attribute: str):\n",
    "\n",
    "    latent_reference = getattr(reference, attribute)\n",
    "    latent_target = getattr(target, attribute)\n",
    "    diff = latent_target.size(-1) - latent_reference.size(-1)\n",
    "\n",
    "    print(\"Adding Noise to \" + attribute + \" for ground truth\")\n",
    "    if latent_reference.shape != latent_target.shape:\n",
    "        print(f\"Shape mismatch with ground_truth={latent_reference.shape}, target={latent_target.shape}\")\n",
    "\n",
    "    else:\n",
    "        if (latent_reference.dim() < 3) and (latent_target.dim() < 3):\n",
    "            latent_reference = latent_reference.unsqueeze(1)\n",
    "            latent_target = latent_target.unsqueeze(1)\n",
    "\n",
    "        noise = torch.randn(*latent_reference.shape[:-1], diff, device=latent_reference.device, dtype=latent_reference.dtype)\n",
    "\n",
    "        latent_reference = torch.cat([latent_reference, noise], dim=-1)\n",
    "\n",
    "    return latent_reference * (1 - interpolation_percentage) + latent_target * interpolation_percentage"
   ],
   "id": "ddaf2ada5513118a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def addNumber(to_change: torch.Tensor, reference: torch.Tensor, number: float):\n",
    "\n",
    "    diff = reference.size(-1) - to_change.size(-1)\n",
    "\n",
    "    if diff < 0:\n",
    "        print(\"Reference is smaller then whats to be changed\")\n",
    "        return to_change\n",
    "\n",
    "    zeros = torch.full((*to_change.shape[:-1], diff), number, device=to_change.device, dtype=to_change.dtype)\n",
    "\n",
    "    to_change = torch.cat([to_change, zeros], dim=-1)\n",
    "\n",
    "    return to_change"
   ],
   "id": "5a473e15209e1e05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def resizeAttribute(to_change, reference):\n",
    "\n",
    "    if reference.dim() == 2 and to_change.dim() == 2:\n",
    "        reference = reference.unsqueeze(0)\n",
    "        to_change = to_change.unsqueeze(0)\n",
    "\n",
    "    resized = F.interpolate(\n",
    "        to_change,\n",
    "        size=reference.shape[-1],\n",
    "        mode='linear',\n",
    "        align_corners=False\n",
    "    )\n",
    "\n",
    "    resized = resized.squeeze(0)\n",
    "\n",
    "    return resized"
   ],
   "id": "86c15635d319d37b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classes",
   "id": "30899a9854f5c3ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class InferenceResult:\n",
    "    h_text: torch.Tensor\n",
    "    h_aligned: torch.Tensor\n",
    "    f0_pred: torch.Tensor\n",
    "    a_pred: torch.Tensor\n",
    "    n_pred: torch.Tensor\n",
    "    style_vector_prosodic: torch.Tensor\n",
    "\n",
    "    def save(self, folder: str):\n",
    "\n",
    "        os.makedirs(\"outputs/latent/\"+folder, exist_ok=True)\n",
    "\n",
    "        # Iterate through all fields of the dataclass\n",
    "        for name, value in self.__dict__.items():\n",
    "            if isinstance(value, torch.Tensor):\n",
    "                path = os.path.join(\"outputs/latent/\"+folder, f\"{name}.pt\")\n",
    "                torch.save(value, path)\n",
    "                print(f\"✅ Saved {name} -> {path}\")\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {name} (not a tensor)\")"
   ],
   "id": "64c25aeb35373288",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class StyleTTS2_Helper:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Splits words into phonemes (symbols that represent how words are pronounced)\n",
    "        self.model = None\n",
    "        self.params = None\n",
    "        self.sampler = None\n",
    "\n",
    "        self.global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "            language='en-us',\n",
    "            preserve_punctuation=True,  # Keeps Punctuation such as , . ? !\n",
    "            with_stress=True  # Adds stress marks to vowels\n",
    "        )\n",
    "\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.textcleaner = TextCleaner()  # Lowercasing & trimming, expanding numbers & symbols, handling punctuation, phoneme conversion, tokenization\n",
    "\n",
    "    def load_models(self, yml_path=\"Models/LJSpeech/config.yml\"):\n",
    "        config = yaml.safe_load(open(yml_path))  # YAML File with model settings and pretrained checkpoints (ASR, F0, PL-BERT)\n",
    "\n",
    "        # load pretrained ASR (Automatic Speech Recognition) model\n",
    "        ASR_config = config.get('ASR_config', False)  # YAML config that describes the model’s structure\n",
    "        ASR_path = config.get('ASR_path', False)  # Checkpoint File\n",
    "        text_aligner = load_ASR_models(ASR_path, ASR_config)  # Load PyTorch model\n",
    "\n",
    "        # load pretrained F0 model (Extracts Pitch Features from Audio, How Pitch Changes over time)\n",
    "        F0_path = config.get('F0_path', False)  # YAML config that describes the model’s structure\n",
    "        pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "        # load BERT model (encodes input text with prosodic cues)\n",
    "        BERT_path = config.get('PLBERT_dir', False)  # YAML config that describes the model’s structure\n",
    "        plbert = load_plbert(BERT_path)\n",
    "\n",
    "        self.model = build_model(\n",
    "            recursive_munch(config['model_params']),  # Allows attribute-style access to keys of model_params,\n",
    "            text_aligner,  # Automatic Speech Recognition model\n",
    "            pitch_extractor,  # F0 model\n",
    "            plbert  # BERT model\n",
    "        )\n",
    "\n",
    "        _ = [self.model[key].eval() for key in self.model]\n",
    "        _ = [self.model[key].to(self.device) for key in self.model]\n",
    "\n",
    "        params_whole = torch.load(\"Models/LJSpeech/epoch_2nd_00100.pth\", map_location='cpu')\n",
    "        self.params = params_whole['net']\n",
    "\n",
    "    def load_checkpoints(self):\n",
    "        for key in self.model:\n",
    "            if key in self.params:\n",
    "                try:\n",
    "                    self.model[key].load_state_dict(self.params[key])\n",
    "                except:\n",
    "                    from collections import OrderedDict\n",
    "                    state_dict = self.params[key]\n",
    "                    new_state_dict = OrderedDict()\n",
    "                    for k, v in state_dict.items():\n",
    "                        name = k[7:]  # remove `module.`\n",
    "                        new_state_dict[name] = v\n",
    "                    # load params\n",
    "                    self.model[key].load_state_dict(new_state_dict, strict=False)\n",
    "        #             except:\n",
    "        #                 _load(params[key], model[key])\n",
    "        _ = [self.model[key].eval() for key in self.model]\n",
    "\n",
    "    def sample_diffusion(self):\n",
    "        self.sampler = DiffusionSampler(\n",
    "            self.model.diffusion.diffusion,\n",
    "            sampler=ADPM2Sampler(),\n",
    "            sigma_schedule=KarrasSchedule(sigma_min=0.0001, sigma_max=3.0, rho=9.0),  # empirical parameters\n",
    "            clamp=False\n",
    "        )\n",
    "\n",
    "    # Turns text to tensor with token ID\n",
    "    def preprocessText(self, text):\n",
    "        # 1. Preprocessing Text\n",
    "        text = text.strip()  # Removes whitespaces from beginning and end of string\n",
    "        text = text.replace('\"', '')  # removes \" to prevent unpredictable behavior\n",
    "\n",
    "        # 2. Text -> Phoneme\n",
    "        phonemes = self.global_phonemizer.phonemize([text])  # text -> list of phoneme\n",
    "        phonemes = word_tokenize(phonemes[0])  # Split into individual tokens\n",
    "        phonemes = ' '.join(phonemes)  # Join tokens together, split by a empty space\n",
    "\n",
    "        # 3. Phoneme -> Token ID\n",
    "        tokens = self.textcleaner(phonemes)  # Look up numeric ID per phoneme\n",
    "        tokens.insert(0, 0)  # Insert leading 0 to mark start\n",
    "\n",
    "        # 4. Token ID -> PyTorch Tensor\n",
    "        tokens = torch.LongTensor(tokens).to(self.device).unsqueeze(0)  # Converts numeric ID to PyTorch Tensor\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def predictDuration(self, bert_encoder_with_style, input_lengths):\n",
    "\n",
    "        # Duration Predictor, frames per phoneme\n",
    "        d_pred, _ = self.model.predictor.lstm(bert_encoder_with_style)  # Model temporal dependencies between phonemes, LSTM = RNN\n",
    "        d_pred = self.model.predictor.duration_proj(d_pred)  # Predict how long each phoneme lasts\n",
    "        d_pred = torch.sigmoid(d_pred).sum(axis=-1)  # Sum of duration prediction -> Result: Prediction of frame duration\n",
    "        d_pred = torch.round(d_pred.squeeze()).clamp(min=1)  # Convert duration prediction into integers, add clamp to ensure that each phoneme has at least one frame\n",
    "        d_pred[-1] += 5  # Makes last phoneme last 5 frames longer, to ensure it not being cut off too fast\n",
    "\n",
    "        # Creates predicted alignment matrix between text (phonemes) and audio frames\n",
    "        a_pred = torch.zeros(input_lengths, int(d_pred.sum().data))  # Initializes a matrix with sizes: [# of Phonemes (input_lengths)] x [Sum of total predicted frames]\n",
    "        current_frame = 0\n",
    "        for i in range(a_pred.size(0)):  # Iterates over phoneme\n",
    "            a_pred[i, current_frame:current_frame + int(d_pred[i].data)] = 1  # Changes for row-i (the i-th phoneme) all the values from current_frame to current_frame + int(d_pred[i].data) to 1\n",
    "            current_frame += int(d_pred[i].data)  # Move current_frame to new first start\n",
    "\n",
    "        return a_pred\n",
    "\n",
    "    def computeStyleVector(self, noise, h_bert, embedding_scale, diffusion_steps):\n",
    "\n",
    "        style_vector = self.sampler(\n",
    "            noise,\n",
    "            embedding=h_bert[0].unsqueeze(0),\n",
    "            embedding_scale=embedding_scale,\n",
    "            num_steps=diffusion_steps\n",
    "        ).squeeze(0)\n",
    "\n",
    "        # Split Style Vector\n",
    "        style_vector_acoustic = style_vector[:, 128:]  # Right Half = Acoustic Style Vector\n",
    "        style_vector_prosodic = style_vector[:, :128]  # Left Half = Prosodic Style Vector\n",
    "\n",
    "        return style_vector_acoustic, style_vector_prosodic\n",
    "\n",
    "    def inference(self, text, noise, diffusion_steps=5, embedding_scale=1):\n",
    "\n",
    "        # Ground Truth\n",
    "        tokens = self.preprocessText(text)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            input_lengths = torch.LongTensor([tokens.shape[-1]]).to(tokens.device)  # Number of phoneme / Length of tokens, shape[-1] = last element in list/array\n",
    "            text_mask = length_to_mask(input_lengths).to(tokens.device)  # Creates a bitmask based on number of phonemes\n",
    "\n",
    "            h_text = self.model.text_encoder(tokens, input_lengths, text_mask)  # Creates acoustic text encoder (phoneme -> feature vectors)\n",
    "            h_bert = self.model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "            bert_encoder = self.model.bert_encoder(h_bert).transpose(-1, -2)  # Creates prosodic text encoder (phoneme -> feature vectors)\n",
    "\n",
    "            ## Function Call\n",
    "            style_vector_acoustic, style_vector_prosodic = self.computeStyleVector(noise, h_bert, embedding_scale, diffusion_steps)\n",
    "\n",
    "            # AdaIN, Adding information of style vector to phoneme\n",
    "            bert_encoder_with_style = self.model.predictor.text_encoder(bert_encoder, style_vector_acoustic, input_lengths, text_mask)\n",
    "\n",
    "            ## Function Call\n",
    "            a_pred = self.predictDuration(bert_encoder_with_style, input_lengths)\n",
    "\n",
    "            # Multiply alignment matrix with h_text\n",
    "            h_aligned = h_text @ a_pred.unsqueeze(0).to(self.device)  # (B, D_text, T_frames)\n",
    "\n",
    "            # encode prosody\n",
    "            bert_encoder_with_style_per_frame = (bert_encoder_with_style.transpose(-1, -2) @ a_pred.unsqueeze(0).to(self.device))  # Multiply per-phoneme embedding (bert_encoder_with_style) with frame-per-phoneme matrix -> per-frame text embedding\n",
    "            f0_pred, n_pred = self.model.predictor.F0Ntrain(bert_encoder_with_style_per_frame, style_vector_acoustic)\n",
    "\n",
    "        return InferenceResult(\n",
    "            h_text=h_text,\n",
    "            h_aligned=h_aligned,\n",
    "            f0_pred=f0_pred,\n",
    "            a_pred=a_pred,\n",
    "            n_pred=n_pred,\n",
    "            style_vector_prosodic=style_vector_prosodic,\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def synthesizeSpeech(self, inferenceResult):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.model.decoder(\n",
    "                inferenceResult.h_aligned,\n",
    "                inferenceResult.f0_pred,\n",
    "                inferenceResult.n_pred,\n",
    "                inferenceResult.style_vector_prosodic.squeeze().unsqueeze(0)\n",
    "            )\n",
    "\n",
    "        return out.squeeze().cpu().numpy()"
   ],
   "id": "20b93c8c80c62433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main Function",
   "id": "3c0877c714b1dc97"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Models",
   "id": "41fbddf28ddd91a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pipe = StyleTTS2_Helper()\n",
    "pipe.load_models()  # builds self.model and loads self.params\n",
    "pipe.load_checkpoints()  # puts params into self.model\n",
    "pipe.sample_diffusion()  # builds self.sampler"
   ],
   "id": "9c5ec47cdf42b30e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Initializes Values",
   "id": "568d95151e9e2684"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diffusion_steps = 5\n",
    "embedding_scale = 1\n",
    "\n",
    "interpolation_percentage = 0.45 # How much of Target to be used, small interpolation_percentage means more of ground_truth (Minimization)\n",
    "\n",
    "name_gt = \"ground_truth\"\n",
    "text_gt = \"This is a medium-length sentence, maybe it will work out?\"\n",
    "\n",
    "name_target = \"target\"\n",
    "text_target = \"This is a longer sentence to see how the model copes with different lengths\"\n",
    "\n",
    "noise_gt = torch.randn(1, 1, 256).to(pipe.device)\n",
    "noise_target = torch.randn(1, 1, 256).to(pipe.device)\n",
    "\n",
    "tokens_gt = pipe.preprocessText(text_gt)\n",
    "tokens_target = pipe.preprocessText(text_target)\n",
    "print(\"tokens_gt.shape (before zeros):\", tokens_gt.shape)\n",
    "print(\"tokens_gt (before zeros):\", tokens_gt)\n",
    "tokens_gt = addNumber(tokens_gt, tokens_target, 16)\n",
    "print(\"tokens_gt.shape (after zeros):\", tokens_gt.shape)\n",
    "print(\"tokens_gt (after zeros):\", tokens_gt)\n",
    "print(\"tokens_target.shape :\", tokens_target.shape)\n",
    "print(\"tokens_target:\", tokens_target)"
   ],
   "id": "20a82d0850930c51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "9f12f43af0a3741b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with torch.no_grad():\n",
    "    # Number of phoneme / Length of tokens, shape[-1] = last element in list/array\n",
    "    input_lengths_gt = torch.LongTensor([tokens_gt.shape[-1]]).to(tokens_gt.device)\n",
    "    input_lengths_target = torch.LongTensor([tokens_target.shape[-1]]).to(tokens_target.device)\n",
    "\n",
    "    # Creates a bitmask based on number of phonemes\n",
    "    text_mask_gt = length_to_mask(input_lengths_gt).to(tokens_gt.device)\n",
    "    text_mask_target = length_to_mask(input_lengths_target).to(tokens_target.device)\n",
    "\n",
    "    # Creates acoustic text encoder (phoneme -> feature vectors)\n",
    "    print(\"\\n h_text:\")\n",
    "    h_text_gt = pipe.model.text_encoder(tokens_gt, input_lengths_gt, text_mask_gt)\n",
    "    print(h_text_gt)\n",
    "    h_text_target = pipe.model.text_encoder(tokens_target, input_lengths_target, text_mask_target)\n",
    "    print(h_text_target)\n",
    "\n",
    "    # Creates prosodic text encoder (phoneme -> feature vectors)\n",
    "    h_bert_gt = pipe.model.bert(tokens_gt, attention_mask=(~text_mask_gt).int())\n",
    "    h_bert_target = pipe.model.bert(tokens_target, attention_mask=(~text_mask_target).int())\n",
    "    bert_encoder_gt = pipe.model.bert_encoder(h_bert_gt).transpose(-1, -2)\n",
    "    bert_encoder_target = pipe.model.bert_encoder(h_bert_target).transpose(-1, -2)\n",
    "\n",
    "    ## Function Call\n",
    "    style_vector_gt_acoustic, style_vector_gt_prosodic = pipe.computeStyleVector(noise_gt, h_bert_gt, embedding_scale, diffusion_steps)\n",
    "    style_vector_target_acoustic, style_vector_target_prosodic = pipe.computeStyleVector(noise_target, h_bert_target, embedding_scale, diffusion_steps)\n",
    "\n",
    "    # AdaIN, Adding information of style vector to phoneme\n",
    "    bert_encoder_gt_with_style = pipe.model.predictor.text_encoder(bert_encoder_gt, style_vector_gt_acoustic, input_lengths_gt, text_mask_gt)\n",
    "    bert_encoder_target_with_style = pipe.model.predictor.text_encoder(bert_encoder_target, style_vector_target_acoustic, input_lengths_target, text_mask_target)\n",
    "\n",
    "    ## Function Call\n",
    "    a_pred_gt = pipe.predictDuration(bert_encoder_gt_with_style, input_lengths_gt)\n",
    "    a_pred_target = pipe.predictDuration(bert_encoder_target_with_style, input_lengths_target)\n",
    "    # a_pred_mixed = resizeAttribute(a_pred_target, a_pred_gt)\n",
    "    print(\"\\na_pred_gt:\", a_pred_gt.shape)\n",
    "    print(\"a_pred_target:\", a_pred_target.shape)\n",
    "    print(\"a_pred_mixed:\", a_pred_mixed.shape)\n",
    "\n",
    "    # Multiply alignment matrix with h_text\n",
    "    h_aligned_gt = h_text_gt @ a_pred_gt.unsqueeze(0).to(pipe.device)  # (B, D_text, T_frames)\n",
    "    h_aligned_target = h_text_target @ a_pred_target.unsqueeze(0).to(pipe.device)\n",
    "    # h_aligned_mixed = h_text_gt @ a_pred_mixed.unsqueeze(0).to(pipe.device)  # (B, D_text, T_frames)\n",
    "    # h_aligned_mixed = (1 - interpolation_percentage) * h_aligned_mixed + interpolation_percentage * h_aligned_target\n",
    "    print(\"\\nh_aligned_gt:\", h_aligned_gt.shape)\n",
    "    print(\"h_aligned_target:\", h_aligned_target.shape)\n",
    "    # print(\"h_aligned_mixed:\", h_aligned_mixed.shape)\n",
    "\n",
    "    # Multiply per-phoneme embedding (bert_encoder_with_style) with frame-per-phoneme matrix -> per-frame text embedding\n",
    "    bert_encoder_gt_with_style_per_frame = (bert_encoder_gt_with_style.transpose(-1, -2) @ a_pred_gt.unsqueeze(0).to(pipe.device))\n",
    "    bert_encoder_target_with_style_per_frame = (bert_encoder_target_with_style.transpose(-1, -2) @ a_pred_target.unsqueeze(0).to(pipe.device))\n",
    "    # bert_encoder_mixed_with_style_per_frame = (bert_encoder_gt_with_style.transpose(-1, -2) @ a_pred_mixed.unsqueeze(0).to(pipe.device))\n",
    "\n",
    "    f0_pred_gt, n_pred_gt = pipe.model.predictor.F0Ntrain(bert_encoder_gt_with_style_per_frame, style_vector_gt_acoustic)\n",
    "    f0_pred_target, n_pred_target = pipe.model.predictor.F0Ntrain(bert_encoder_target_with_style_per_frame, style_vector_target_acoustic)\n",
    "    # f0_pred_mixed, n_pred_mixed = pipe.model.predictor.F0Ntrain(bert_encoder_mixed_with_style_per_frame, style_vector_gt_acoustic)\n",
    "\n",
    "inferenceResult_gt = InferenceResult(\n",
    "    h_text=h_text_gt,\n",
    "    h_aligned=h_aligned_gt,\n",
    "    f0_pred=f0_pred_gt,\n",
    "    a_pred=a_pred_gt,\n",
    "    n_pred=n_pred_gt,\n",
    "    style_vector_prosodic=style_vector_gt_prosodic,\n",
    ")\n",
    "\n",
    "inferenceResult_target = InferenceResult(\n",
    "    h_text=h_text_target,\n",
    "    h_aligned=h_aligned_target,\n",
    "    f0_pred=f0_pred_target,\n",
    "    a_pred=a_pred_target,\n",
    "    n_pred=n_pred_target,\n",
    "    style_vector_prosodic=style_vector_target_prosodic,\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "inferenceResult_mixed = InferenceResult(\n",
    "    h_text=h_text_gt,\n",
    "    h_aligned=h_aligned_mixed,\n",
    "    f0_pred=f0_pred_mixed,\n",
    "    a_pred=a_pred_gt,\n",
    "    n_pred=n_pred_mixed,\n",
    "    style_vector_prosodic=style_vector_gt_prosodic,\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "audio_gt = pipe.synthesizeSpeech(inferenceResult_gt)\n",
    "audio_target = pipe.synthesizeSpeech(inferenceResult_target)\n",
    "# audio_mixed = pipe.synthesizeSpeech(inferenceResult_mixed)\n",
    "\n",
    "print(\"\\nground truth\")\n",
    "display(ipd.Audio(audio_gt, rate=24000))\n",
    "print(\"target\")\n",
    "display(ipd.Audio(audio_target, rate=24000))\n",
    "# print(\"mixed\")\n",
    "# display(ipd.Audio(audio_mixed, rate=24000))\n",
    "\n",
    "sf.write(\"outputs/audio/padding_empty_phonemes_target.wav\", audio_gt, samplerate=24000)\n",
    "sf.write(\"outputs/audio/padding_empty_phonemes_reference.wav\", audio_target, samplerate=24000)\n"
   ],
   "id": "a70096d9dfc96373",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "analyzeAudio(audio_gt)\n",
    "analyzeAudio(audio_target)\n",
    "analyzeAudio(audio_mixed)"
   ],
   "id": "6939684df43ad144",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
